{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8babd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a981ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628078a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4feb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        u,b,r = l.strip().split(',')\n",
    "        r = int(r)\n",
    "        yield u,b,r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b780f4",
   "metadata": {},
   "source": [
    "# Read Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e0690",
   "metadata": {},
   "source": [
    "## Setting up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f30eb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "allRatings = []\n",
    "for l in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    allRatings.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "041120af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('u67805239', 'b61372131', 4)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allRatings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f0150ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "026569e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsTrain = allRatings[:190000]\n",
    "ratingsValid = allRatings[190000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b385be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "usersPerItem = defaultdict(set) # Maps an item to the users who rated it\n",
    "itemsPerUser = defaultdict(set)\n",
    "ratingDict = {}\n",
    "\n",
    "for u,b,r in ratingsTrain:\n",
    "    ratingsPerUser[u].append((b,r))\n",
    "    ratingsPerItem[b].append((u,r))\n",
    "    usersPerItem[b].add(u)\n",
    "    itemsPerUser[u].add(b)\n",
    "    ratingDict[(u,b)] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "94a449da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6868052631578947"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRatings = [r[2] for r in ratingsTrain]\n",
    "globalAverage = sum(trainRatings) * 1.0 / len(trainRatings)\n",
    "globalAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "70c8df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemAverages = {}\n",
    "userAverages = {}\n",
    "\n",
    "for i in ratingsPerItem:\n",
    "    rs = [r[1] for r in ratingsPerItem[i]]\n",
    "    itemAverages[i] = sum(rs) / len(rs)\n",
    "\n",
    "for u in ratingsPerUser: \n",
    "    rs = [r[1] for r in ratingsPerUser[u]]\n",
    "    userAverages[u] = sum(rs) / len(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "53694b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from baseline code\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user,book,_ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > (3 * totalRead)/4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ec0ef194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add neg entry for every user to validation set\n",
    "userBooks = defaultdict(list)\n",
    "userNoBooks = defaultdict(list)\n",
    "\n",
    "for u,b,r in allRatings:  \n",
    "    userBooks[u].append(b)\n",
    "\n",
    "for u in userBooks: \n",
    "    for b in bookCount: \n",
    "        if b not in userBooks[u]: \n",
    "            userNoBooks[u].append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6c53162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add neg entry for every user to validation set\n",
    "ratingsToAdd = []\n",
    "for u,b,_ in ratingsValid: \n",
    "    \n",
    "    rand = random.randrange(len(userNoBooks[u]))\n",
    "    book = userNoBooks[u][rand]\n",
    "    userNoBooks[u].remove(book)\n",
    "    \n",
    "    ratingsToAdd.append((u,book,-1))\n",
    "    userBooks[u].append(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8ab93d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsValid.extend(ratingsToAdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7fd48",
   "metadata": {},
   "source": [
    "## Trying feature vector + log regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "58bb7f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('u67805239', 'b61372131', 4)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsTrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e847227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum): \n",
    "    feat = [1]\n",
    "    \n",
    "    user = datum[0]\n",
    "    item = datum[1]\n",
    "    r = datum[2]\n",
    "    \n",
    "    # popularity of item \n",
    "    count = 0\n",
    "    popT = 0.0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        if i == item: \n",
    "            popT = count / totalRead\n",
    "            break \n",
    "    \n",
    "    # MAX jaccard sim for item \n",
    "    maxIjaccard = 0.0\n",
    "    if user in ratingsPerUser: \n",
    "        jaccardList = []\n",
    "        for pair in ratingsPerUser[user]:\n",
    "            b2 = pair[0]\n",
    "            if item == b2: \n",
    "                #alreadyRead = True\n",
    "                #break\n",
    "                continue\n",
    "\n",
    "            # get users who read the book\n",
    "            readB = usersPerItem[item]\n",
    "            readB2 = usersPerItem[b2]\n",
    "\n",
    "            # compute similarity between the two books \n",
    "            sim = Jaccard(readB,readB2)\n",
    "            jaccardList.append(sim)\n",
    "\n",
    "        jaccardList.sort(reverse=True)\n",
    "        if len(jaccardList) >= 1: \n",
    "            maxIjaccard = jaccardList[0]\n",
    "        \n",
    "    \n",
    "    # Max jaccard sim for users \n",
    "    maxUjaccard = 0.0\n",
    "    if item in ratingsPerItem:\n",
    "        jaccardList = []\n",
    "        for pair in ratingsPerItem[item]:\n",
    "            u2 = pair[0]\n",
    "            if user == u2: \n",
    "                #alreadyRead = True\n",
    "                #break\n",
    "                continue\n",
    "\n",
    "            # get books for each user \n",
    "            readU = itemsPerUser[user]\n",
    "            readU2 = itemsPerUser[u2]\n",
    "\n",
    "            # compute similarity between the two users\n",
    "            sim = Jaccard(readU,readU2)\n",
    "            jaccardList.append(sim)\n",
    "      \n",
    "        jaccardList.sort(reverse=True)\n",
    "        if len(jaccardList) >= 1: \n",
    "            maxUjaccard = jaccardList[0]\n",
    "    \n",
    "    #if popT != 0: \n",
    "        #popT = 1 / popT \n",
    "        \n",
    "    return feat + [popT, maxIjaccard, maxUjaccard]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "dec3c5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0.091275, 0.015355086372360844, 0.14814814814814814]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature(ratingsTrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2822d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on validation set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3b0137bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 20000/20000 [00:13<00:00, 1434.50it/s]\n"
     ]
    }
   ],
   "source": [
    "Xvalid = [feature(d) for d in tqdm(ratingsValid)]\n",
    "Yvalid = [not(d[2] == -1) for d in ratingsValid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "526f5aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "mod = linear_model.LogisticRegression(C=43.0, class_weight='balanced', verbose=True)\n",
    "mod.fit(Xvalid,Yvalid)\n",
    "predictions = mod.predict(Xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f71e4e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False,  True,  True,  True,  True, False,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "eb12976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7058 8064 1936 2942 0.2439 0.7561\n"
     ]
    }
   ],
   "source": [
    "TP = sum([a == b and b == True for a,b in zip(predictions,Yvalid)])\n",
    "TN = sum([a == b and b == False for a,b in zip(predictions,Yvalid)])\n",
    "FP = sum([a != b and b == False for a,b in zip(predictions,Yvalid)])\n",
    "FN = sum([a != b and b == True for a,b in zip(predictions,Yvalid)])\n",
    "acc = [a == b for a,b in zip(predictions, Yvalid)]\n",
    "acc = sum(acc) / len(acc)\n",
    "\n",
    "BTP = TP / (TP + FN)\n",
    "BTN = TN / (TN + FP)\n",
    "BER = 1 - (BTP + BTN) / 2\n",
    "\n",
    "print(TP,TN,FP,FN,BER, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f09f1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for best c \n",
    "vals = np.arange(1.0, 50.0, 1.0)\n",
    "maxAcc = 0.0 \n",
    "t = 1.0 \n",
    "for c in vals:\n",
    "    mod = linear_model.LogisticRegression(C=c, class_weight='balanced')\n",
    "    mod.fit(Xvalid,Yvalid)\n",
    "    predictions = mod.predict(Xvalid)\n",
    "    \n",
    "    acc = [a == b for a,b in zip(predictions, Yvalid)]\n",
    "    acc = sum(acc) / len(acc)\n",
    "    \n",
    "    if acc > maxAcc: \n",
    "        maxAcc = acc\n",
    "        t = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b0834fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7561 43.0\n"
     ]
    }
   ],
   "source": [
    "print(maxAcc, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "02ba94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Read.csv\", 'w')\n",
    "\n",
    "for l in open(\"pairs_Read.csv\"):\n",
    "    #print(l)\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split(',')\n",
    "    # (etc.)\n",
    "    \n",
    "    length += 1\n",
    "    feat = feature((u,b,-1000)) # rating doesnt matter \n",
    "    result = mod.predict([feat])\n",
    "    \n",
    "    pred = 0\n",
    "    if result == True: \n",
    "        pred = 1\n",
    "\n",
    "    line = u + \",\" + b + \",\" + str(pred) + \"\\n\"\n",
    "    predictions.write(line)\n",
    "\n",
    "predictions.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2decc4f7",
   "metadata": {},
   "source": [
    "###### IDEAS: \n",
    "\n",
    "1. use both, improve similarity \n",
    "    - rather than looking at the maximum similarity of a book, try avg similarity or avg similarity among 10% of total similarity entries or min \n",
    "2. try different similarity methods\n",
    "3. try similarity based on Users, not items \n",
    "3. make feature vectors including \n",
    "    - popularity \n",
    "    - similarity \n",
    "    - length of word "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b683ec",
   "metadata": {},
   "source": [
    "## Conclusions: \n",
    "\n",
    "1. Basic pop + sim using max(Jaccard) is very effective\n",
    "2. not much difference between max(Jaccard) and avg(Jaccard) \n",
    "3. Jaccard sim over items > over users \n",
    "4. Feature vector has given best accuracy -> Winning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937465a",
   "metadata": {},
   "source": [
    "# Category Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77555423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b36992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "04475b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "reviewsPerUser = defaultdict(list)\n",
    "\n",
    "for d in readGz(\"train_Category.json.gz\"):\n",
    "    u = d['user_id']\n",
    "    r = d['review_id']\n",
    "    \n",
    "    reviewsPerUser[u].append(d)\n",
    "\n",
    "    data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672a11f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'u75242413',\n",
       " 'review_id': 'r45843137',\n",
       " 'rating': 4,\n",
       " 'review_text': \"a clever book with a deeply troubling premise and an intriguing protagonist. Thompson's clean, sparse prose style kept each page feeling light even as some rather heavy existential questions dropped upon them. I enjoyed it. \\n and that cover design is boom-pow gorgeous.\",\n",
       " 'n_votes': 1,\n",
       " 'genre': 'mystery_thriller_crime',\n",
       " 'genreID': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9fdd68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewTrain = data[:90000]\n",
    "reviewValid = data[90000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "999b7d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'u88632301',\n",
       " 'review_id': 'r75997435',\n",
       " 'rating': 4,\n",
       " 'review_text': 'Great book! Well written and very believeable. Enjoyed that it was set in a small town north of Albuquerque--brought back many memories. The language is realistic and not too over the top. Same theme but totally different setting than VA is used to.',\n",
       " 'n_votes': 0,\n",
       " 'genre': 'young_adult',\n",
       " 'genreID': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewTrain[50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c485e75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~'}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "punctuation.remove('!')\n",
    "punctuation.remove('?')\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "2a7100f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CREATIVE VERSION \n",
    "stop = stopwords.words(\"english\")\n",
    "wordCount = defaultdict(int)\n",
    "\n",
    "wordSetPerReview = defaultdict(set)\n",
    "for d in reviewTrain: \n",
    "    u = d['user_id']\n",
    "    r_id = d['review_id']\n",
    "    \n",
    "    for w in d['review_text'].split(): \n",
    "        r = [\"\".join([c for c in w.lower() if not c in punctuation])]\n",
    "        \n",
    "        # addressing ! and ? \n",
    "        if '!' in w: \n",
    "            r = r[0].split('!')\n",
    "            r.append('!')\n",
    "            \n",
    "        if '?' in w: \n",
    "            r = r[0].split('?')\n",
    "            r.append('?')\n",
    "            \n",
    "       \n",
    "        for word in r:    \n",
    "            if word in stop: \n",
    "                continue \n",
    "            wordSetPerReview[r_id].add(word)\n",
    "            wordCount[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "3ff5d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram version \n",
    "stop = stopwords.words(\"english\")\n",
    "wordCount = defaultdict(int)\n",
    "for d in reviewTrain: \n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "    ws1 = []\n",
    "    \n",
    "    for w in ws: \n",
    "        \n",
    "        if '!' in w: \n",
    "            w2 = w.split('!')\n",
    "            w2[1] = '!'\n",
    "        elif '?' in w: \n",
    "            w2 = w.split('?')\n",
    "            w2[1] = '?' \n",
    "        else: \n",
    "            w2 = [w]\n",
    "            \n",
    "        for word in w2: \n",
    "            if word in stop: \n",
    "                continue \n",
    "            ws1.append(word)\n",
    "               \n",
    "    #ws1 = [w for w in ws if w not in stop]\n",
    "    ws2 = [' '.join(x) for x in list(zip(ws1[:-1],ws1[1:]))]\n",
    "    #ws3 = [' '.join(x) for x in list(zip(ws1[:-2],ws1[1:-1],ws1[2:]))]\n",
    "    #ws4 = [' '.join(x) for x in list(zip(ws1[:-3],ws1[1:-2],ws1[2:-1],ws1[3:]))]\n",
    "    #ws5 = [' '.join(x) for x in list(zip(ws[:-4],ws[1:-3],ws[2:-2],ws[3:-1],ws[4:]))]\n",
    "    for w in ws1 + ws2: #+ ws3: #+ ws4 + ws5:\n",
    "        wordCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "57656fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'clever': 1310,\n",
       "             'book': 130717,\n",
       "             'deeply': 795,\n",
       "             'troubling': 65,\n",
       "             'premise': 1730,\n",
       "             'intriguing': 1980,\n",
       "             'protagonist': 2059,\n",
       "             'thompsons': 34,\n",
       "             'clean': 546,\n",
       "             'sparse': 79,\n",
       "             'prose': 1023,\n",
       "             'style': 4300,\n",
       "             'kept': 4669,\n",
       "             'page': 4915,\n",
       "             'feeling': 3479,\n",
       "             'light': 2809,\n",
       "             'even': 22451,\n",
       "             'rather': 4448,\n",
       "             'heavy': 862,\n",
       "             'existential': 56,\n",
       "             'questions': 2283,\n",
       "             'dropped': 380,\n",
       "             'upon': 1657,\n",
       "             'enjoyed': 12882,\n",
       "             'cover': 3113,\n",
       "             'design': 225,\n",
       "             'boompow': 1,\n",
       "             'gorgeous': 790,\n",
       "             'clever book': 25,\n",
       "             'book deeply': 12,\n",
       "             'deeply troubling': 3,\n",
       "             'troubling premise': 1,\n",
       "             'premise intriguing': 17,\n",
       "             'intriguing protagonist': 1,\n",
       "             'protagonist thompsons': 1,\n",
       "             'thompsons clean': 1,\n",
       "             'clean sparse': 1,\n",
       "             'sparse prose': 2,\n",
       "             'prose style': 17,\n",
       "             'style kept': 8,\n",
       "             'kept page': 2,\n",
       "             'page feeling': 1,\n",
       "             'feeling light': 2,\n",
       "             'light even': 9,\n",
       "             'even rather': 3,\n",
       "             'rather heavy': 6,\n",
       "             'heavy existential': 2,\n",
       "             'existential questions': 5,\n",
       "             'questions dropped': 1,\n",
       "             'dropped upon': 1,\n",
       "             'upon enjoyed': 1,\n",
       "             'enjoyed cover': 2,\n",
       "             'cover design': 10,\n",
       "             'design boompow': 1,\n",
       "             'boompow gorgeous': 1,\n",
       "             'little': 19165,\n",
       "             'much': 27498,\n",
       "             'retconning': 7,\n",
       "             'honest': 4520,\n",
       "             'wolverines': 8,\n",
       "             'past': 5281,\n",
       "             'mostly': 2112,\n",
       "             'mystery': 8833,\n",
       "             'part': 9679,\n",
       "             'content': 907,\n",
       "             'saying': 1767,\n",
       "             'formed': 246,\n",
       "             'protoxmen': 1,\n",
       "             'group': 2486,\n",
       "             'doesnt': 9658,\n",
       "             'feel': 12082,\n",
       "             'right': 9017,\n",
       "             'neither': 717,\n",
       "             'xavier': 64,\n",
       "             'plays': 712,\n",
       "             'far': 5728,\n",
       "             'didnt': 20003,\n",
       "             'think': 20245,\n",
       "             'really': 42410,\n",
       "             'established': 284,\n",
       "             'school': 6317,\n",
       "             'crippled': 47,\n",
       "             'little much': 190,\n",
       "             'much retconning': 1,\n",
       "             'retconning honest': 1,\n",
       "             'honest wolverines': 1,\n",
       "             'wolverines past': 1,\n",
       "             'past mostly': 5,\n",
       "             'mostly mystery': 3,\n",
       "             'mystery part': 53,\n",
       "             'part content': 1,\n",
       "             'content saying': 1,\n",
       "             'saying formed': 1,\n",
       "             'formed protoxmen': 1,\n",
       "             'protoxmen group': 1,\n",
       "             'group doesnt': 3,\n",
       "             'doesnt feel': 189,\n",
       "             'feel right': 60,\n",
       "             'right neither': 3,\n",
       "             'neither part': 2,\n",
       "             'part xavier': 1,\n",
       "             'xavier plays': 1,\n",
       "             'plays far': 1,\n",
       "             'far didnt': 9,\n",
       "             'didnt think': 493,\n",
       "             'think really': 194,\n",
       "             'really established': 1,\n",
       "             'established school': 1,\n",
       "             'school crippled': 1,\n",
       "             'glad': 3201,\n",
       "             'finally': 4340,\n",
       "             'got': 9825,\n",
       "             'around': 7580,\n",
       "             'reading': 22824,\n",
       "             'tammara': 5,\n",
       "             'webber': 16,\n",
       "             'officially': 184,\n",
       "             'favorite': 6296,\n",
       "             'authorwell': 1,\n",
       "             'next': 12535,\n",
       "             'jane': 1123,\n",
       "             'austen': 199,\n",
       "             'course': 4409,\n",
       "             'im': 23559,\n",
       "             'rest': 3526,\n",
       "             'btl': 11,\n",
       "             'series': 33823,\n",
       "             '!': 58052,\n",
       "             'glad finally': 113,\n",
       "             'finally got': 232,\n",
       "             'got around': 111,\n",
       "             'around reading': 109,\n",
       "             'reading book': 2264,\n",
       "             'book tammara': 1,\n",
       "             'tammara webber': 3,\n",
       "             'webber officially': 1,\n",
       "             'officially favorite': 4,\n",
       "             'favorite authorwell': 1,\n",
       "             'authorwell next': 1,\n",
       "             'next jane': 2,\n",
       "             'jane austen': 127,\n",
       "             'austen course': 1,\n",
       "             'course im': 31,\n",
       "             'im reading': 268,\n",
       "             'reading rest': 174,\n",
       "             'rest btl': 1,\n",
       "             'btl series': 1,\n",
       "             'series !': 1087,\n",
       "             'would': 28474,\n",
       "             'classify': 74,\n",
       "             'erotic': 299,\n",
       "             'paranormal': 2407,\n",
       "             'almost': 5192,\n",
       "             'stopped': 736,\n",
       "             'whole': 7249,\n",
       "             'needing': 220,\n",
       "             'save': 2106,\n",
       "             'kid': 1466,\n",
       "             'redeemed': 135,\n",
       "             'self': 1115,\n",
       "             'revealed': 1170,\n",
       "             'fake': 292,\n",
       "             'memory': 981,\n",
       "             'sentences': 489,\n",
       "             'later': 3070,\n",
       "             'quick': 2922,\n",
       "             'entertaining': 2681,\n",
       "             'read': 62877,\n",
       "             'would classify': 17,\n",
       "             'classify erotic': 1,\n",
       "             'erotic paranormal': 13,\n",
       "             'paranormal almost': 2,\n",
       "             'almost stopped': 25,\n",
       "             'stopped reading': 139,\n",
       "             'reading whole': 65,\n",
       "             'whole needing': 2,\n",
       "             'needing save': 2,\n",
       "             'save kid': 1,\n",
       "             'kid part': 1,\n",
       "             'part redeemed': 1,\n",
       "             'redeemed self': 2,\n",
       "             'self revealed': 1,\n",
       "             'revealed fake': 1,\n",
       "             'fake memory': 1,\n",
       "             'memory sentences': 1,\n",
       "             'sentences later': 4,\n",
       "             'later quick': 3,\n",
       "             'quick entertaining': 35,\n",
       "             'entertaining read': 334,\n",
       "             'loved': 17778,\n",
       "             'butcher': 204,\n",
       "             'although': 5671,\n",
       "             'still': 16399,\n",
       "             'dresden': 351,\n",
       "             'fits': 464,\n",
       "             'writing': 11092,\n",
       "             'better': 9834,\n",
       "             'informally': 3,\n",
       "             'written': 8715,\n",
       "             'epic': 1331,\n",
       "             'fantasy': 6046,\n",
       "             'characters': 36523,\n",
       "             'great': 19573,\n",
       "             'though': 14203,\n",
       "             'story': 60202,\n",
       "             'gotten': 743,\n",
       "             'start': 6634,\n",
       "             'plot': 13624,\n",
       "             'line': 3094,\n",
       "             'seems': 5820,\n",
       "             'obvious': 1413,\n",
       "             'dont': 18437,\n",
       "             'mind': 4712,\n",
       "             'loved !': 774,\n",
       "             '! butcher': 2,\n",
       "             'butcher although': 1,\n",
       "             'although still': 80,\n",
       "             'still think': 210,\n",
       "             'think dresden': 3,\n",
       "             'dresden fits': 1,\n",
       "             'fits writing': 1,\n",
       "             'writing style': 1647,\n",
       "             'style better': 3,\n",
       "             'better little': 13,\n",
       "             'little informally': 1,\n",
       "             'informally written': 1,\n",
       "             'written epic': 9,\n",
       "             'epic fantasy': 211,\n",
       "             'fantasy characters': 25,\n",
       "             'characters great': 284,\n",
       "             'great though': 31,\n",
       "             'though story': 119,\n",
       "             'story gotten': 5,\n",
       "             'gotten great': 3,\n",
       "             'great start': 140,\n",
       "             'start plot': 10,\n",
       "             'plot line': 320,\n",
       "             'line seems': 3,\n",
       "             'seems little': 84,\n",
       "             'little obvious': 13,\n",
       "             'obvious dont': 5,\n",
       "             'dont really': 581,\n",
       "             'really mind': 23,\n",
       "             'originally': 1121,\n",
       "             'posted': 825,\n",
       "             'httpiliveforreadingblogspotcom2': 2,\n",
       "             'one': 55785,\n",
       "             'high': 4503,\n",
       "             'toread': 116,\n",
       "             'list': 1953,\n",
       "             'mean': 3921,\n",
       "             'hype': 429,\n",
       "             'illuminae': 67,\n",
       "             'huge': 2277,\n",
       "             'couldnt': 6004,\n",
       "             'go': 11242,\n",
       "             'twitter': 221,\n",
       "             'feed': 221,\n",
       "             'without': 6698,\n",
       "             'seeing': 2601,\n",
       "             'mentions': 286,\n",
       "             'every': 8339,\n",
       "             'day': 6351,\n",
       "             'goodreads': 1241,\n",
       "             'absolutely': 3573,\n",
       "             'abuzz': 5,\n",
       "             'title': 1906,\n",
       "             'waiting': 1756,\n",
       "             'wednesday': 30,\n",
       "             'post': 573,\n",
       "             'seemed': 4547,\n",
       "             'feature': 335,\n",
       "             'surprised': 2175,\n",
       "             'received': 2888,\n",
       "             'copy': 3884,\n",
       "             'mail': 136,\n",
       "             'timing': 120,\n",
       "             'tooit': 2,\n",
       "             'arrived': 239,\n",
       "             'left': 6137,\n",
       "             'airport': 130,\n",
       "             'heading': 244,\n",
       "             'second': 6791,\n",
       "             'year': 4901,\n",
       "             'university': 330,\n",
       "             'talk': 2064,\n",
       "             'good': 27689,\n",
       "             'luck': 407,\n",
       "             'unfortunately': 1832,\n",
       "             'get': 24551,\n",
       "             'semesterthere': 1,\n",
       "             'wasnt': 9909,\n",
       "             'enough': 8745,\n",
       "             'time': 25224,\n",
       "             'work': 7774,\n",
       "             'way': 21933,\n",
       "             'shame': 500,\n",
       "             'amazing': 5870,\n",
       "             'ive': 9419,\n",
       "             'previously': 504,\n",
       "             'kaufmans': 9,\n",
       "             'broken': 1234,\n",
       "             'stars': 9525,\n",
       "             'another': 10455,\n",
       "             'collab': 2,\n",
       "             'novel': 16432,\n",
       "             'wrote': 1199,\n",
       "             'meagan': 21,\n",
       "             'spooner': 10,\n",
       "             'since': 7235,\n",
       "             'set': 5433,\n",
       "             'space': 1079,\n",
       "             'friggin': 26,\n",
       "             'awesome': 3098,\n",
       "             'knew': 3741,\n",
       "             'world': 17717,\n",
       "             'literally': 1352,\n",
       "             'friend': 5060,\n",
       "             'borrowed': 156,\n",
       "             'constantly': 1093,\n",
       "             'gushed': 12,\n",
       "             'nonstop': 245,\n",
       "             'fast': 3490,\n",
       "             '2016': 420,\n",
       "             'fantastic': 3007,\n",
       "             'nothing': 5546,\n",
       "             'books': 26364,\n",
       "             'could': 16407,\n",
       "             'join': 505,\n",
       "             'format': 662,\n",
       "             'mindblowing': 65,\n",
       "             'never': 12783,\n",
       "             'fun': 9444,\n",
       "             'bookwell': 4,\n",
       "             'okay': 2824,\n",
       "             'serious': 1404,\n",
       "             'parts': 3792,\n",
       "             'went': 3537,\n",
       "             'beyond': 1465,\n",
       "             'expectation': 139,\n",
       "             'however': 7349,\n",
       "             'sitting': 1104,\n",
       "             'also': 22463,\n",
       "             'spent': 1326,\n",
       "             'ages': 872,\n",
       "             'looking': 6200,\n",
       "             'details': 2255,\n",
       "             'pages': 6379,\n",
       "             'mockwiki': 1,\n",
       "             'articles': 78,\n",
       "             'chats': 32,\n",
       "             'censored': 14,\n",
       "             'material': 564,\n",
       "             'brilliant': 1858,\n",
       "             'realistic': 2062,\n",
       "             'sticking': 149,\n",
       "             'protocol': 22,\n",
       "             'many': 13723,\n",
       "             'blacked': 8,\n",
       "             'words': 3507,\n",
       "             'per': 2124,\n",
       "             'bit': 13782,\n",
       "             'sad': 2779,\n",
       "             'missing': 2426,\n",
       "             'artwork': 779,\n",
       "             'internal': 357,\n",
       "             'tears': 801,\n",
       "             'shed': 713,\n",
       "             'tk': 18,\n",
       "             'appear': 681,\n",
       "             'ah': 291,\n",
       "             'oh': 3618,\n",
       "             'well': 22624,\n",
       "             'means': 1897,\n",
       "             'finished': 3335,\n",
       "             'wink': 78,\n",
       "             'kady': 41,\n",
       "             'ezra': 95,\n",
       "             'kickass': 264,\n",
       "             'love': 32806,\n",
       "             'actually': 7839,\n",
       "             'starts': 2886,\n",
       "             'breaking': 503,\n",
       "             'rare': 668,\n",
       "             'see': 17979,\n",
       "             'female': 2605,\n",
       "             'male': 1509,\n",
       "             'counterpart': 61,\n",
       "             'interests': 320,\n",
       "             'general': 1278,\n",
       "             'relationship': 5695,\n",
       "             'liked': 12229,\n",
       "             'kaufman': 32,\n",
       "             'kristoff': 42,\n",
       "             'pretty': 7962,\n",
       "             'handle': 631,\n",
       "             'teens': 1182,\n",
       "             'message': 1190,\n",
       "             'otherit': 1,\n",
       "             'made': 12108,\n",
       "             'ims': 14,\n",
       "             'various': 1310,\n",
       "             'forms': 252,\n",
       "             'communication': 154,\n",
       "             'lot': 13523,\n",
       "             'learn': 3259,\n",
       "             'quite': 8499,\n",
       "             'interesting': 12487,\n",
       "             'positions': 66,\n",
       "             'onboard': 22,\n",
       "             'two': 14606,\n",
       "             'ships': 189,\n",
       "             'aidan': 106,\n",
       "             'super': 2003,\n",
       "             'creepy': 1376,\n",
       "             'cool': 1861,\n",
       "             'major': 1714,\n",
       "             '2001': 47,\n",
       "             'odyssey': 31,\n",
       "             'feels': 3370,\n",
       "             'theres': 6744,\n",
       "             'reference': 371,\n",
       "             'embedded': 44,\n",
       "             'mixed': 1015,\n",
       "             'people': 14674,\n",
       "             'random': 746,\n",
       "             'names': 1136,\n",
       "             'promoted': 36,\n",
       "             'demoted': 13,\n",
       "             'ranks': 100,\n",
       "             'changedit': 1,\n",
       "             'confusing': 1302,\n",
       "             'side': 3902,\n",
       "             'big': 5133,\n",
       "             'twists': 3476,\n",
       "             'come': 8602,\n",
       "             'waythings': 1,\n",
       "             'seem': 4000,\n",
       "             'totally': 2996,\n",
       "             'turn': 2754,\n",
       "             'different': 10024,\n",
       "             'end': 17346,\n",
       "             'true': 4134,\n",
       "             'seriously': 2082,\n",
       "             'idea': 4533,\n",
       "             'whats': 1976,\n",
       "             'going': 15078,\n",
       "             'happen': 3872,\n",
       "             'deadly': 554,\n",
       "             'plague': 226,\n",
       "             'breaks': 556,\n",
       "             'creates': 657,\n",
       "             'problem': 2763,\n",
       "             'leads': 1091,\n",
       "             'terrifying': 463,\n",
       "             'consequences': 622,\n",
       "             'like': 49975,\n",
       "             'said': 5327,\n",
       "             'ai': 239,\n",
       "             'fascinating': 1743,\n",
       "             'reactions': 342,\n",
       "             'ship': 718,\n",
       "             'heard': 1698,\n",
       "             'snippets': 123,\n",
       "             'tidbits': 105,\n",
       "             'hopefully': 764,\n",
       "             'tell': 4053,\n",
       "             'pumped': 34,\n",
       "             'experience': 2463,\n",
       "             'none': 1257,\n",
       "             'othera': 2,\n",
       "             'gripping': 847,\n",
       "             'survival': 570,\n",
       "             'presented': 621,\n",
       "             'incredible': 972,\n",
       "             'visual': 248,\n",
       "             'created': 2032,\n",
       "             'readers': 5659,\n",
       "             'wont': 3330,\n",
       "             'able': 4384,\n",
       "             'put': 7519,\n",
       "             'cant': 12205,\n",
       "             'wait': 6366,\n",
       "             'trilogy': 2502,\n",
       "             'returning': 280,\n",
       "             'ezras': 14,\n",
       "             'perspectives': 495,\n",
       "             'gain': 347,\n",
       "             'insight': 691,\n",
       "             'files': 250,\n",
       "             'events': 2984,\n",
       "             'transpired': 32,\n",
       "             'fall': 2485,\n",
       "             'cannot': 2268,\n",
       "             'thank': 1674,\n",
       "             'aisha': 3,\n",
       "             'house': 3087,\n",
       "             'childrens': 977,\n",
       "             'sending': 248,\n",
       "             'review': 12069,\n",
       "             'originally posted': 407,\n",
       "             'posted httpiliveforreadingblogspotcom2': 2,\n",
       "             'httpiliveforreadingblogspotcom2 one': 1,\n",
       "             'one high': 34,\n",
       "             'high toread': 2,\n",
       "             'toread list': 74,\n",
       "             'list mean': 3,\n",
       "             'mean hype': 2,\n",
       "             'hype illuminae': 1,\n",
       "             'illuminae huge': 1,\n",
       "             'huge couldnt': 2,\n",
       "             'couldnt go': 32,\n",
       "             'go twitter': 1,\n",
       "             'twitter feed': 7,\n",
       "             'feed without': 1,\n",
       "             'without seeing': 14,\n",
       "             'seeing mentions': 1,\n",
       "             'mentions book': 5,\n",
       "             'book every': 118,\n",
       "             'every day': 409,\n",
       "             'day goodreads': 1,\n",
       "             'goodreads absolutely': 2,\n",
       "             'absolutely abuzz': 1,\n",
       "             'abuzz title': 1,\n",
       "             'title every': 4,\n",
       "             'every waiting': 1,\n",
       "             'waiting wednesday': 6,\n",
       "             'wednesday post': 2,\n",
       "             'post seemed': 1,\n",
       "             'seemed feature': 2,\n",
       "             'feature book': 15,\n",
       "             'book really': 1763,\n",
       "             'really surprised': 101,\n",
       "             'surprised received': 1,\n",
       "             'received copy': 548,\n",
       "             'copy mail': 2,\n",
       "             'mail great': 1,\n",
       "             'great timing': 2,\n",
       "             'timing tooit': 1,\n",
       "             'tooit arrived': 1,\n",
       "             'arrived left': 1,\n",
       "             'left airport': 1,\n",
       "             'airport heading': 1,\n",
       "             'heading second': 1,\n",
       "             'second year': 9,\n",
       "             'year university': 8,\n",
       "             'university talk': 1,\n",
       "             'talk good': 11,\n",
       "             'good luck': 41,\n",
       "             'luck !': 14,\n",
       "             '! unfortunately': 36,\n",
       "             'unfortunately though': 13,\n",
       "             'though couldnt': 39,\n",
       "             'couldnt even': 103,\n",
       "             'even get': 173,\n",
       "             'get read': 175,\n",
       "             'read book': 4489,\n",
       "             'book semesterthere': 1,\n",
       "             'semesterthere wasnt': 1,\n",
       "             'wasnt enough': 186,\n",
       "             'enough time': 110,\n",
       "             'time work': 20,\n",
       "             'work heading': 1,\n",
       "             'heading way': 7,\n",
       "             'way shame': 2,\n",
       "             'shame got': 1,\n",
       "             'got one': 104,\n",
       "             'one amazing': 45,\n",
       "             'amazing ive': 10,\n",
       "             'ive previously': 8,\n",
       "             'previously read': 50,\n",
       "             'read kaufmans': 1,\n",
       "             'kaufmans broken': 1,\n",
       "             'broken stars': 25,\n",
       "             'stars another': 20,\n",
       "             'another collab': 1,\n",
       "             'collab novel': 1,\n",
       "             'novel wrote': 3,\n",
       "             'wrote meagan': 1,\n",
       "             'meagan spooner': 5,\n",
       "             'spooner since': 1,\n",
       "             'since set': 10,\n",
       "             'set space': 19,\n",
       "             'space friggin': 1,\n",
       "             'friggin awesome': 4,\n",
       "             'awesome knew': 4,\n",
       "             'knew one': 40,\n",
       "             'one would': 413,\n",
       "             'would world': 14,\n",
       "             'world literally': 10,\n",
       "             'literally friend': 1,\n",
       "             'friend borrowed': 1,\n",
       "             'borrowed one': 3,\n",
       "             'one constantly': 9,\n",
       "             'constantly gushed': 1,\n",
       "             'gushed nonstop': 1,\n",
       "             'nonstop knew': 1,\n",
       "             'knew read': 44,\n",
       "             'read one': 1405,\n",
       "             'one fast': 17,\n",
       "             'fast far': 3,\n",
       "             'far 2016': 2,\n",
       "             '2016 fantastic': 2,\n",
       "             'fantastic start': 14,\n",
       "             'start ive': 6,\n",
       "             'ive reading': 151,\n",
       "             'reading nothing': 12,\n",
       "             'nothing amazing': 13,\n",
       "             'amazing books': 34,\n",
       "             'books glad': 11,\n",
       "             'glad one': 22,\n",
       "             'one could': 253,\n",
       "             'could join': 5,\n",
       "             'join list': 6,\n",
       "             'list !': 65,\n",
       "             '! format': 5,\n",
       "             'format mindblowing': 1,\n",
       "             'mindblowing ive': 1,\n",
       "             'ive never': 579,\n",
       "             'never much': 36,\n",
       "             'much fun': 396,\n",
       "             'fun reading': 160,\n",
       "             'reading bookwell': 1,\n",
       "             'bookwell okay': 1,\n",
       "             'okay wasnt': 17,\n",
       "             'wasnt much': 250,\n",
       "             'fun got': 3,\n",
       "             'got serious': 6,\n",
       "             'serious parts': 5,\n",
       "             'parts format': 1,\n",
       "             'format went': 1,\n",
       "             'went beyond': 20,\n",
       "             'beyond every': 3,\n",
       "             'every expectation': 5,\n",
       "             'expectation however': 1,\n",
       "             'however book': 187,\n",
       "             'book huge': 58,\n",
       "             'couldnt get': 484,\n",
       "             'get one': 188,\n",
       "             'one sitting': 486,\n",
       "             'sitting also': 2,\n",
       "             'also spent': 8,\n",
       "             'spent absolutely': 1,\n",
       "             'absolutely ages': 3,\n",
       "             'ages looking': 1,\n",
       "             'looking details': 3,\n",
       "             'details pages': 3,\n",
       "             'pages mockwiki': 1,\n",
       "             'mockwiki articles': 1,\n",
       "             'articles chats': 1,\n",
       "             'chats censored': 1,\n",
       "             'censored material': 1,\n",
       "             'material brilliant': 1,\n",
       "             'brilliant realistic': 1,\n",
       "             'realistic sticking': 1,\n",
       "             'sticking protocol': 1,\n",
       "             'protocol fun': 1,\n",
       "             'fun seeing': 23,\n",
       "             'seeing many': 14,\n",
       "             'many blacked': 1,\n",
       "             'blacked words': 1,\n",
       "             'words per': 7,\n",
       "             'per page': 19,\n",
       "             'page bit': 2,\n",
       "             'bit sad': 67,\n",
       "             'sad pages': 1,\n",
       "             'pages missing': 5,\n",
       "             'missing artwork': 2,\n",
       "             'artwork internal': 1,\n",
       "             'internal tears': 1,\n",
       "             'tears shed': 15,\n",
       "             'shed words': 1,\n",
       "             'words artwork': 3,\n",
       "             'artwork tk': 1,\n",
       "             'tk would': 1,\n",
       "             'would appear': 31,\n",
       "             'appear page': 4,\n",
       "             'page ah': 1,\n",
       "             'ah oh': 2,\n",
       "             'oh well': 187,\n",
       "             'well means': 8,\n",
       "             'means get': 16,\n",
       "             'get finished': 14,\n",
       "             'finished copy': 16,\n",
       "             'copy wink': 1,\n",
       "             'wink wink': 17,\n",
       "             'wink kady': 1,\n",
       "             'kady ezra': 16,\n",
       "             'ezra kickass': 1,\n",
       "             'kickass characters': 4,\n",
       "             'characters love': 221,\n",
       "             'love story': 1466,\n",
       "             'story actually': 130,\n",
       "             'actually starts': 13,\n",
       "             'starts breaking': 5,\n",
       "             'breaking rare': 1,\n",
       "             'rare see': 8,\n",
       "             'see book': 242,\n",
       "             'book female': 19,\n",
       "             'female male': 14,\n",
       "             'male counterpart': 3,\n",
       "             'counterpart love': 1,\n",
       "             'love interests': 117,\n",
       "             'interests general': 1,\n",
       "             'general relationship': 2,\n",
       "             'relationship liked': 10,\n",
       "             'liked kaufman': 1,\n",
       "             'kaufman kristoff': 5,\n",
       "             'kristoff pretty': 1,\n",
       "             'pretty good': 879,\n",
       "             'good handle': 7,\n",
       "             'handle teens': 1,\n",
       "             'teens talk': 4,\n",
       "             'talk message': 1,\n",
       "             'message otherit': 1,\n",
       "             'otherit made': 1,\n",
       "             'made reading': 44,\n",
       "             'reading ims': 1,\n",
       "             'ims various': 1,\n",
       "             'various forms': 9,\n",
       "             'forms communication': 1,\n",
       "             'communication whole': 1,\n",
       "             'whole lot': 384,\n",
       "             'lot better': 189,\n",
       "             'better also': 34,\n",
       "             'also get': 232,\n",
       "             'get learn': 60,\n",
       "             'learn quite': 9,\n",
       "             'quite characters': 15,\n",
       "             'characters interesting': 378,\n",
       "             'interesting positions': 3,\n",
       "             'positions onboard': 1,\n",
       "             'onboard two': 1,\n",
       "             'two ships': 1,\n",
       "             'ships aidan': 1,\n",
       "             'aidan actually': 1,\n",
       "             'actually super': 7,\n",
       "             'super creepy': 21,\n",
       "             'creepy cool': 6,\n",
       "             'cool got': 5,\n",
       "             'got major': 4,\n",
       "             'major 2001': 1,\n",
       "             '2001 space': 3,\n",
       "             'space odyssey': 3,\n",
       "             'odyssey feels': 1,\n",
       "             'feels actually': 2,\n",
       "             'actually think': 84,\n",
       "             'think theres': 99,\n",
       "             'theres reference': 2,\n",
       "             'reference embedded': 1,\n",
       "             'embedded however': 1,\n",
       "             'however get': 33,\n",
       "             'get mixed': 18,\n",
       "             'mixed people': 3,\n",
       "             'people time': 26,\n",
       "             'time time': 288,\n",
       "             'time random': 5,\n",
       "             'random im': 1,\n",
       "             'im names': 1,\n",
       "             'names various': 1,\n",
       "             'various people': 22,\n",
       "             'people got': 25,\n",
       "             'got promoted': 1,\n",
       "             'promoted demoted': 1,\n",
       "             'demoted ranks': 1,\n",
       "             'ranks changedit': 1,\n",
       "             'changedit made': 1,\n",
       "             'made confusing': 6,\n",
       "             'confusing side': 1,\n",
       "             'side really': 22,\n",
       "             'really big': 69,\n",
       "             'big twists': 9,\n",
       "             'twists come': 12,\n",
       "             'come waythings': 1,\n",
       "             'waythings seem': 1,\n",
       "             'seem one': 14,\n",
       "             'one way': 247,\n",
       "             'way totally': 15,\n",
       "             'totally turn': 2,\n",
       "             'turn different': 3,\n",
       "             'different end': 15,\n",
       "             'end true': 10,\n",
       "             'true twists': 2,\n",
       "             'twists !': 27,\n",
       "             '! seriously': 106,\n",
       "             'seriously idea': 6,\n",
       "             'idea whats': 23,\n",
       "             'whats going': 464,\n",
       "             'going happen': 660,\n",
       "             'happen deadly': 1,\n",
       "             'deadly plague': 2,\n",
       "             'plague breaks': 1,\n",
       "             'breaks creates': 1,\n",
       "             'creates major': 1,\n",
       "             'major problem': 28,\n",
       "             'problem leads': 1,\n",
       "             'leads terrifying': 1,\n",
       "             'terrifying consequences': 1,\n",
       "             'consequences like': 3,\n",
       "             'like said': 357,\n",
       "             'said aidan': 1,\n",
       "             'aidan super': 2,\n",
       "             'super interesting': 31,\n",
       "             'interesting read': 481,\n",
       "             'read pages': 41,\n",
       "             'pages ai': 1,\n",
       "             'ai fascinating': 1,\n",
       "             'fascinating read': 60,\n",
       "             'read reactions': 2,\n",
       "             'reactions people': 4,\n",
       "             'people ship': 9,\n",
       "             'ship also': 10,\n",
       "             'get heard': 1,\n",
       "             'heard little': 4,\n",
       "             'little snippets': 14,\n",
       "             'snippets tidbits': 1,\n",
       "             'tidbits hopefully': 1,\n",
       "             'hopefully tell': 1,\n",
       "             'tell pumped': 1,\n",
       "             'pumped book': 1,\n",
       "             'book illuminae': 4,\n",
       "             'illuminae experience': 1,\n",
       "             'experience like': 19,\n",
       "             'like none': 16,\n",
       "             'none othera': 1,\n",
       "             'othera gripping': 1,\n",
       "             'gripping story': 66,\n",
       "             'story survival': 24,\n",
       "             'survival presented': 1,\n",
       "             'presented incredible': 1,\n",
       "             'incredible visual': 1,\n",
       "             'visual format': 2,\n",
       "             'format kaufman': 1,\n",
       "             'kristoff created': 4,\n",
       "             'created epic': 3,\n",
       "             'epic story': 43,\n",
       "             'story readers': 32,\n",
       "             'readers wont': 16,\n",
       "             'wont able': 122,\n",
       "             'able put': 144,\n",
       "             'put cant': 20,\n",
       "             'cant wait': 3451,\n",
       "             'wait read': 1330,\n",
       "             'read rest': 380,\n",
       "             'rest trilogy': 42,\n",
       "             'trilogy even': 11,\n",
       "             'even though': 3641,\n",
       "             'though wont': 19,\n",
       "             'wont returning': 3,\n",
       "             'returning kady': 1,\n",
       "             'kady ezras': 1,\n",
       "             'ezras perspectives': 1,\n",
       "             'perspectives well': 9,\n",
       "             'well gain': 2,\n",
       "             'gain insight': 14,\n",
       "             'insight world': 17,\n",
       "             'world illuminae': 1,\n",
       "             'illuminae files': 3,\n",
       "             'files events': 1,\n",
       "             'events transpired': 11,\n",
       "             'transpired novel': 1,\n",
       "             'novel perspectives': 1,\n",
       "             'perspectives fall': 1,\n",
       "             'fall cannot': 1,\n",
       "             'cannot get': 60,\n",
       "             'get fast': 15,\n",
       "             'fast enough': 129,\n",
       "             'enough !': 95,\n",
       "             '! thank': 118,\n",
       "             'thank much': 69,\n",
       "             'much aisha': 1,\n",
       "             'aisha random': 1,\n",
       "             'random house': 76,\n",
       "             'house childrens': 7,\n",
       "             'childrens books': 154,\n",
       "             'books sending': 3,\n",
       "             'sending copy': 20,\n",
       "             'copy review': 52,\n",
       "             'review !': 172,\n",
       "             'miss': 1566,\n",
       "             'changelings': 33,\n",
       "             'excited': 2036,\n",
       "             'back': 12937,\n",
       "             'snowdancers': 4,\n",
       "             'den': 1695,\n",
       "             'last': 8435,\n",
       "             'ok': 1983,\n",
       "             'passion': 637,\n",
       "             'good book': 1558,\n",
       "             'book great': 816,\n",
       "             'great one': 122,\n",
       "             'one good': 235,\n",
       "             'good miss': 4,\n",
       "             'miss changelings': 1,\n",
       "             'changelings im': 1,\n",
       "             'im super': 49,\n",
       "             'super excited': 85,\n",
       "             'excited get': 64,\n",
       "             'get back': 523,\n",
       "             'back snowdancers': 1,\n",
       "             'snowdancers den': 1,\n",
       "             'den last': 1,\n",
       "             'last books': 65,\n",
       "             'books ok': 7,\n",
       "             'ok dont': 16,\n",
       "             'dont passion': 2,\n",
       "             'passion books': 5,\n",
       "             'books changelings': 1,\n",
       "             'shel': 26,\n",
       "             'silverstein': 20,\n",
       "             'classic': 1465,\n",
       "             'stretch': 217,\n",
       "             'giraffe': 25,\n",
       "             'half': 3642,\n",
       "             '?': 32429,\n",
       "             'goes': 4150,\n",
       "             'add': 1826,\n",
       "             'things': 15682,\n",
       "             'rat': 182,\n",
       "             'hat': 1280,\n",
       "             'cute': 2869,\n",
       "             'suit': 219,\n",
       "             'poor': 1271,\n",
       "             'overwhelmed': 176,\n",
       "             'backwards': 107,\n",
       "             'shel silverstein': 16,\n",
       "             'silverstein classic': 1,\n",
       "             'classic stretch': 1,\n",
       "             'stretch giraffe': 1,\n",
       "             'giraffe stretch': 1,\n",
       "             'stretch another': 1,\n",
       "             'another half': 12,\n",
       "             'half ?': 7,\n",
       "             '? giraffe': 1,\n",
       "             'giraffe half': 2,\n",
       "             'half story': 85,\n",
       "             'story goes': 202,\n",
       "             'goes add': 1,\n",
       "             'add things': 9,\n",
       "             'things giraffe': 1,\n",
       "             'half like': 5,\n",
       "             'like rat': 1,\n",
       "             'rat hat': 1,\n",
       "             'hat cute': 1,\n",
       "             'cute suit': 1,\n",
       "             'suit goes': 1,\n",
       "             'goes poor': 1,\n",
       "             'poor giraffe': 1,\n",
       "             'giraffe overwhelmed': 1,\n",
       "             'overwhelmed story': 3,\n",
       "             'goes backwards': 1,\n",
       "             'sayall': 1,\n",
       "             'booktubeyeah': 1,\n",
       "             'lived': 1050,\n",
       "             'iti': 53,\n",
       "             'wanna': 202,\n",
       "             'hug': 174,\n",
       "             'kiss': 726,\n",
       "             'let': 4258,\n",
       "             'goits': 1,\n",
       "             'simple': 2148,\n",
       "             'beautifuljust': 1,\n",
       "             'tiny': 655,\n",
       "             'thingi': 4,\n",
       "             'understand': 4469,\n",
       "             'real': 7926,\n",
       "             'life': 16921,\n",
       "             'isnt': 6277,\n",
       "             'always': 9271,\n",
       "             'happy': 4343,\n",
       "             'ending': 9483,\n",
       "             'trying': 5593,\n",
       "             'show': 3199,\n",
       "             'deserves': 660,\n",
       "             'eleanor': 315,\n",
       "             'park': 693,\n",
       "             'mean sayall': 1,\n",
       "             'sayall hype': 1,\n",
       "             'hype booktubeyeah': 1,\n",
       "             'booktubeyeah lived': 1,\n",
       "             'lived iti': 1,\n",
       "             'iti wanna': 1,\n",
       "             'wanna hug': 3,\n",
       "             'hug kiss': 2,\n",
       "             'kiss never': 4,\n",
       "             'never let': 85,\n",
       "             'let book': 33,\n",
       "             'book goits': 1,\n",
       "             'goits simple': 1,\n",
       "             'simple beautifuljust': 1,\n",
       "             'beautifuljust one': 1,\n",
       "             'one tiny': 22,\n",
       "             'tiny thingi': 1,\n",
       "             'thingi understand': 1,\n",
       "             'understand real': 7,\n",
       "             'real life': 733,\n",
       "             'life isnt': 83,\n",
       "             'isnt always': 87,\n",
       "             'always happy': 37,\n",
       "             'happy ending': 495,\n",
       "             ...})"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "c1e6b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount.pop('', None)\n",
    "mostPopular = [(wordCount[w], w) for w in wordCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "533002eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3876250"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "b7b9457d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(130717, 'book'),\n",
       " (62877, 'read'),\n",
       " (60202, 'story'),\n",
       " (58052, '!'),\n",
       " (55785, 'one'),\n",
       " (49975, 'like'),\n",
       " (42410, 'really'),\n",
       " (36523, 'characters'),\n",
       " (33823, 'series'),\n",
       " (32806, 'love')]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostPopular[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "f6f1ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N gram feature \n",
    "def feature8N(datum): \n",
    "    f = [0]*len(wordSet)\n",
    "    datumWordSet = set()\n",
    "    \n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "    ws1 = []\n",
    "    \n",
    "    for w in ws: \n",
    "        if '!' in w: \n",
    "            w2 = w.split('!')\n",
    "            w2.append('!')\n",
    "        elif '?' in w: \n",
    "            w2 = w.split('?')\n",
    "            w2.append('?')\n",
    "        else: \n",
    "            w2 = [w]\n",
    "            \n",
    "        for word in w2: \n",
    "            if word in stop: \n",
    "                continue \n",
    "            ws1.append(word)\n",
    "            \n",
    "    #ws1 = [w for w in ws if w not in stop]\n",
    "    ws2 = [' '.join(x) for x in list(zip(ws1[:-1],ws1[1:]))]\n",
    "    #ws3 = [' '.join(x) for x in list(zip(ws1[:-2],ws1[1:-1],ws1[2:]))]\n",
    "    #ws4 = [' '.join(x) for x in list(zip(ws[:-3],ws[1:-2],ws[2:-1],ws[3:]))]\n",
    "    #ws5 = [' '.join(x) for x in list(zip(ws[:-4],ws[1:-3],ws[2:-2],ws[3:-1],ws[4:]))]\n",
    "    \n",
    "    for w in ws1 + ws2: #+ ws3: #+ ws4 + ws5:\n",
    "        if w in wordSet:\n",
    "            index = wordId[w]\n",
    "            f[index] += 1\n",
    "            \n",
    "            datumWordSet.add(word)\n",
    "    \n",
    "    # somehow leverage user history \n",
    "    avgRating = [0.0]*5\n",
    "    u = datum['user_id']\n",
    "    simPerGenre = [-1.0]*5 \n",
    "    if u in reviewsPerUser: \n",
    "        # find avg rating user has given for each genre and check similarities of words in reviews, take max for each genre \n",
    "        reviews = reviewsPerUser[u]\n",
    "        numReviews = [0]*5\n",
    "        \n",
    "        \n",
    "        for rev in reviews: \n",
    "            if rev['review_id'] == datum['review_id']: \n",
    "                continue\n",
    "            rating = rev['rating']\n",
    "            ind = rev['genreID']\n",
    "            \n",
    "            # avg stuff \n",
    "            avgRating[ind] += rating \n",
    "            numReviews[ind] += 1\n",
    "            \n",
    "            # similarity stuff\n",
    "            revWordSet = wordSetPerReview[rev['review_id']]\n",
    "            \n",
    "            if len(revWordSet) == 0 and len(datumWordSet) == 0: \n",
    "                sim = 0.0 \n",
    "                #print(rev['review_id'], datum['review_id'])\n",
    "            else: \n",
    "                sim = Jaccard(revWordSet, datumWordSet)\n",
    "        \n",
    "            simPerGenre[ind] = max(sim, simPerGenre[ind])\n",
    "\n",
    "        # more avg stuff \n",
    "        for val in range(0,5): \n",
    "            if numReviews[val] != 0: \n",
    "                avgRating[val] = avgRating[val] / numReviews[val]\n",
    "    \n",
    "    \n",
    "    return f + avgRating + simPerGenre + [1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "462992d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numerator = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    \n",
    "    return numerator / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "890740ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creative feature \n",
    "def feature8C(datum): \n",
    "    f = [0]*len(wordSet)\n",
    "    datumWordSet = set()\n",
    "    \n",
    "    for w in datum['review_text'].split(): \n",
    "        r = [\"\".join([c for c in w.lower() if not c in punctuation])]\n",
    "        \n",
    "        # addressing ! and ? \n",
    "        if '!' in w: \n",
    "            r = r[0].split('!')\n",
    "            r.append('!')\n",
    "        \n",
    "        if '?' in w: \n",
    "            r = r[0].split('?')\n",
    "            r.append('?')\n",
    "            \n",
    "        #if w in stop: \n",
    "            #continue \n",
    "        for word in r:  \n",
    "            if word in wordSet: \n",
    "                index = wordId[word]\n",
    "                f[index] += 1\n",
    "                \n",
    "                datumWordSet.add(word)\n",
    "    \n",
    "    # somehow leverage user history \n",
    "    avgRating = [0.0]*5\n",
    "    u = datum['user_id']\n",
    "    simPerGenre = [-1.0]*5 \n",
    "    if u in reviewsPerUser: \n",
    "        # find avg rating user has given for each genre and check similarities of words in reviews, take max for each genre \n",
    "        reviews = reviewsPerUser[u]\n",
    "        numReviews = [0]*5\n",
    "        \n",
    "        \n",
    "        for rev in reviews: \n",
    "            if rev['review_id'] == datum['review_id']: \n",
    "                continue\n",
    "            rating = rev['rating']\n",
    "            ind = rev['genreID']\n",
    "            \n",
    "            # avg stuff \n",
    "            avgRating[ind] += rating \n",
    "            numReviews[ind] += 1\n",
    "            \n",
    "            # similarity stuff\n",
    "            revWordSet = wordSetPerReview[rev['review_id']]\n",
    "            \n",
    "            if len(revWordSet) == 0 and len(datumWordSet) == 0: \n",
    "                sim = 0.0 \n",
    "                #print(rev['review_id'], datum['review_id'])\n",
    "            else: \n",
    "                sim = Jaccard(revWordSet, datumWordSet)\n",
    "        \n",
    "            simPerGenre[ind] = max(sim, simPerGenre[ind])\n",
    "\n",
    "        # more avg stuff \n",
    "        for val in range(0,5): \n",
    "            if numReviews[val] != 0: \n",
    "                avgRating[val] = avgRating[val] / numReviews[val]\n",
    "    \n",
    "    \n",
    "    return f + avgRating + simPerGenre + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "68a7f745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 4.0, 0.0, 0.0, 4.0, -1.0, 0.047619047619047616, -1.0, -1.0, 0.0, 1]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature8C(data[50])[-11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a033ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = np.arange(10000,20000,1000)\n",
    "\n",
    "for val in [10000]: \n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    words = [x[1] for x in mostPopular[:val]]\n",
    "    wordId = dict(zip(words, range(len(words))))\n",
    "    wordSet = set(words)\n",
    "\n",
    "    X = [feature8N(x) for x in data]\n",
    "    y = [x['genreID'] for x in data]\n",
    "\n",
    "    Xtrain = X[:9*len(X)//10]\n",
    "    ytrain = y[:9*len(y)//10]\n",
    "    Xvalid = X[9*len(X)//10:]\n",
    "    yvalid = y[9*len(y)//10:]\n",
    "    \n",
    "    mod = linear_model.LogisticRegression(C=1, verbose=True)\n",
    "    mod.fit(Xtrain,ytrain)\n",
    "\n",
    "    pred = mod.predict(Xvalid)\n",
    "    correct = [(p == l) for (p,l) in zip(pred, yvalid)]\n",
    "    acc7 = sum(correct) / len(correct)\n",
    "    \n",
    "    final = time.perf_counter() - start \n",
    "    \n",
    "    acc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "6e57f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7556 9.164235224999993\n"
     ]
    }
   ],
   "source": [
    "print(acc7, final / 60)\n",
    "\n",
    "# dict size = 5000 \n",
    "# no punctuation but keeping ! / ? = 0.7093 \n",
    "# no punctuation, keeping ! / ?, no stopwords = 0.7189 \n",
    "# no punctuation, = 0.7128 \n",
    "# no punctuation, no stopwords = 0.7169\n",
    "\n",
    "# n-grams \n",
    "    # no stopwords for 1 gram, up to 2 grams - 0.6852\n",
    "    # up to 3 grams - 0.68 \n",
    "    # no stopwords for 1 gram, up to 5 - 0.6779 \n",
    "    \n",
    "    # no stopwards for all grams, up to 2 grams - 0.7066\n",
    "    # \"                         \", up to 2, keeping ! / ? = 0.7049 \n",
    "\n",
    "# dict size = 10000\n",
    "    #-  n gram \"                         \", up to 2, keeping ! / ? = 0.7163 (8 minutes to run)\n",
    "    #-  n gram \"                         \", up to 3, keeping ! / ? = 0.7127 (9 minutes to run)\n",
    "    #-  no punctuation, keeping ! / ?, no stopwords = 0.7344 ()\n",
    "    \n",
    "# extra features \n",
    "    # no punctuation sol + avg rating of genre = 0.7345, 9.5 minutes \n",
    "    # no punc, avg rating, similarity = 0.7556 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927edc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "cfd79861",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "reviewPerId = defaultdict(set)\n",
    "for d in readGz(\"test_Category.json.gz\"):\n",
    "    test.append(d)\n",
    "    revId = d['review_id']\n",
    "    \n",
    "    reviewPerId[revId] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "936fa3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Category.csv\", 'w')\n",
    "pos = 0\n",
    "\n",
    "for l in open(\"pairs_Category.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split(',')\n",
    "    \n",
    "    review = reviewPerId[b]\n",
    "    res = mod.predict([feature8C(review)])\n",
    "    \n",
    "    line = u + \",\" + b + \",\" + str(res[0]) + \"\\n\"\n",
    "    predictions.write(line)\n",
    "    # (etc.)\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0bc6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
